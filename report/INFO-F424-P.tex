\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper,left=2.7cm,right=2.7cm,top=2.7cm,bottom=2.7cm]{geometry}
\usepackage{parskip}
\usepackage{eurosym}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{listings}
\usetikzlibrary{positioning}

%To be able to "escape" listings, example bold.
\lstset{
    escapeinside={(*}{*)}
}

\usepackage{tikz,pgfplots}
\usepackage{csvsimple}

%%%
%%%
%%%
\title{INFO-F424 - Combinatorial Optimization\\Project - The $p$-Center Problem}
\date{\vspace{-3ex}
Erica \textsc{Berghman} \\
Charles \textsc{Hamesse} \\
~\\
École polytechnique de Bruxelles\\
\vspace{6ex}May 2017\vspace{4ex}}
\begin{document}


% You must implement these formulations in Julia language combined with JuMP package.
% You must prepare a project report written in LATEX. In this report you should describe the mathematical formulations referenced above (explaining the meaning of each variable and constraint set), describe the computational experiments, and discuss the results. Performance of the implementation is also taken into consideration in the grading.

\maketitle
\begin{abstract}
    The purpose of this project is to implement two formulations of the same combinatorial optimization problem in Julia, using the JuMP package. We will start by describing the mathematical aspects of both formulations, then we will explain our implementations and discuss their performance.\vspace{2ex}
\end{abstract}

\tableofcontents
\pagebreak

% You must send the report and code to guillerme.duvillie@ulb.ac.be and leave a physical copy at the Secr ́etariat des E ́tudiants du D ́epartment d’Informatique at the 8th floor of the NO building, by 8th of May.

% Ease of use (read/write on the standard input, options, CLI, etc) is taken into consideration in the grading 

%%%
%%%	
%%%

\section{Introduction}
% offering a decent Human Machine Interface. Note that it does not mean that I want a fancy/shiny Graphical Interface. It means, that your program should implement unix like options reading (for instance -h [or --help] for help display, -i [or --input] for file input, ...), stdin reading and stdout writing, different level of verbosity, ... Note however that the program has to be usable in a bash loop/script.
This project is implemented in Julia, a high-level, high-performance dynamic programming language for numerical computing. 

As Julia is mostly a scripting language, nothing needs to be pre-compiled. However, one might have to install the following packages to ensure the correct functioning of our program:
\begin{itemize}
	\item \texttt{ArgParse}: used to parse command-line arguments
	\item \texttt{Cbc}: a simple LP solver
	\item \texttt{Gurobi}: a more complex, commercial LP solver. Only used if requested in \texttt{P1.jl} and \texttt{P3.jl}.
\end{itemize}

To launch the program or use it in a bash script, one only needs to \texttt{cd} to the root directory of our archive and enter:

\begin{lstlisting}
julia main.jl
\end{lstlisting}

We implemented a simple yet complete human-machine interface which is well described using \texttt{-h} or \texttt{--help}:
\begin{lstlisting}
>>> julia main.jl -h
usage: main.jl -i INPUT -f FORMULATION -s SOLVER [-o OUTPUT]
               [-d DIVISOR] [-c INITIAL-CANDIDATE] [-v VERBOSE] [-h]

optional arguments:
  -i, --input, --instance INPUT
                        Path to the instance file.
  -f, --formulation FORMULATION
                        P1 or P3.
  -s, --solver SOLVER   Cbc or Gurobi.
  -o, --output OUTPUT   Output file path. (default: "stdout")
  -d, --divisor DIVISOR
                        Divisor for valid inequality in P1. If set to
                        -1, the new valid inequality is not
                        considered. (default: "-1")
  -c, --initial-candidate INITIAL-CANDIDATE
                        Heuristic for hot start. Use default, random
                        or 2approx. (default: "default")
  -v, --verbose VERBOSE
                        Enable verbose mode. 0 or 1. (default: "0")
  -h, --help            show this help message and exit
\end{lstlisting}

%%%
%%%
%%%
\section{Formulations}
\subsection{Daskin (1995)}
	This is the formulation referred to as (P1) in the original paper.
	
	According to the usual canvas, the mathematical formulation is given as follows.
	\paragraph{Parameters} 
	\begin{eqnarray*}
		p &=& \text{maximum number of centers;} \\
		N &=& \text{number of vertices of the instance.} 
	\end{eqnarray*}
	\paragraph{Variables} Two variables are used in this formulation:
	\begin{eqnarray*}
		y_j &=& \begin{cases}
 				1 ~~\text{if vertex $j$ is a center} \\
 				0 ~~\text{otherwise}
 			\end{cases} \\
 		x_{ij} &=& \begin{cases}
 				1 ~~\text{if vertex $i$ assigns to a center in vertex $j$} \\
 				0 ~~\text{otherwise}
 			\end{cases} \\
	\end{eqnarray*}
	Both indices $i$ and $j$ have a range of $[1, N]$.
	
	\paragraph{Objective function}
	\begin{eqnarray}
		min && z\\
		\text{s.t.}~~~ \sum_{j \in N} d_{ij} x_{ij} &\leq& z \label{eq:2}
	\end{eqnarray}
	These two expressions ensure that the objective value is no less than the maximum vertex-to-center distance, which we want to minimize. Note that (2) is actually implemented as a constraint but shown here for the sake of readability.
	
	\paragraph{Constraints}
	\begin{eqnarray}
		\sum_{j \in N} x_{ij} &=& 1 ~~\forall i \in N \\
		x_{ij} &\leq& y_i ~~\forall i,j \in N \\
		\sum_{j \in N} y_j &\leq& p \\
		y_j &\in& \{ 0,1 \} ~~\forall j \in N \\
		x_{ij} &\in& \{0 , 1 \} ~~\forall i,j \in N 
	\end{eqnarray}
	
	Constraint (3) assigns each vertex to exactly one center.
	Constraint (4) ensures that no vertex assigns to $v_j$ unless there is a center at $v_j$. 
	Constraint (5) restricts the number of centers to $p$.
	Constraints (6) and (7) are the binary restrictions for variables $x$ and $y$.     
    
    \subsection{Calik and Tansel (2013)}
	This is the formulation referred to as (P3) in the original paper. This method uses the fact that the distance $d_{ij}$ are the only possible values for $r_p(F)$. It is thus possible to jump from one $d_{ij}$ to another.
	
	%define correctly r_p(F)
	In this formulation, we define the set $ R = \{ \rho_1, \rho_2, ..., \rho_K \}$ where $\rho_1 < \rho_2 < ... < \rho_K$ is an ordering of the distinct distance values of the matrix of distances $d_{ij}$. One of these values determines the value of $r_p(F)$.
	
	\paragraph{Parameters} 
	\begin{eqnarray*}
		p &=& \text{maximum number of centers;} \\
		N &=& \text{number of vertices of the instance;} \\
		K &=& \text{number of distinct distance values in the instance.}
	\end{eqnarray*}
	
	\paragraph{Variables} Three variables are used in this formulation:
	\begin{eqnarray*}
		a_{ijk} &=& \begin{cases}
 				1 ~~\text{if $d_{ij} \leq \rho_k$} \\
 				0 ~~\text{otherwise}
 			\end{cases} \\
		y_j &=& \begin{cases}
 				1 ~~\text{if vertex $j$ is a center} \\
 				0 ~~\text{otherwise}
 			\end{cases} \\
 		z_{k} &=& \begin{cases}
 				1 ~~\text{if $r_p(F) = \rho_k$} \\
 				0 ~~\text{otherwise}
 			\end{cases} \\
	\end{eqnarray*}
	Both indices $i$ and $j$ have a range of $[1, N]$.
	
	\paragraph{Objective function}
	\begin{eqnarray}
		min && \sum_{k \in T} \rho_{k} z_{k}	
	\end{eqnarray}
	The objective function determines the value of $r_p(F)$ as the corresponding value $\rho_k$.
	
	\paragraph{Constraints}
	\begin{eqnarray}
    	\sum_{j \in M} a_{ijk} y_{j} &\geq& z_k ~~\forall i \in N, \forall k \in T \\
		\sum_{j \in M} y_{j} &\leq& p \\
		\sum_{k \in T} z_{k} &=& 1 \\
		y_j &\in& \{ 0,1 \} ~~\forall j \in M \\
		z_{k} &\in& \{0 , 1 \} ~~\forall k \in T 
	\end{eqnarray}
	
	Constraint (9) ensures that each vertex is covered within the selected radius by at least one center.
	Constraint (10) restricts the number of center to at most p centers.
	Constraint (11) ensures that exactly one of the variables $z_k$ is selected. 
	Constraints (12) and (13) are the binary restrictions for variables $y$ and $z$. 
	
%%%
%%%
%%%
%%%
%%%
%%%


\section{Developer documentation}
\begin{itemize}
    \item[-] \texttt{main.jl} : Entry point of the program
    \item[-] \texttt{P1.jl} : Formulation $P_1$
    \item[-] \texttt{P3.jl} : Formulation $P_3$
    \item[-] \texttt{run\_all\_gurobi\_P1.sh} : Run all instances with formulation $P_1$ with Gurobi as solver 
    \item[-] \texttt{run\_all\_gurobi\_P3.sh} : Run all instances with formulation $P_3$ with Gurobi as solver 
    \item[-] \texttt{chvatal-gomory-divisor-search.sh} and \texttt{chvatal-gomor-divisor-search-2.sh} : Bash loops implementing a grid search to try and find the best divisor value (divisor is explained later on).
    \item[-] \texttt{README.md} : Read me containing primary informations on the project and how to compile
    \item[-] \texttt{instances/} : All the instances to test
    \item[-] \texttt{out/} : Example outputs of some executions
    \item[-] \texttt{report/} : Report (in \LaTeX \; and in PDF)
    \item[-] \texttt{resources/} : Paper and project formulation
    \end{itemize}
    
\subsection{Helper functions} 

\begin{itemize}
\item[-] \texttt{PCInstance.jl} : Type PCInstance, contains 5 attributes: n, p, d, K, rho.
\item[-] \texttt{read\_instance.jl} : Parse an instance (given in path) and returns a PCInstance (Input: path, Output: instance::PCInstance)

\item[-] \texttt{Result.jl} : Type Result, contains 2 attributes: score, time.

\item[-] \texttt{compute\_xij.jl} : Compute the x\_ij based on a solution. (Input: solution::Array{UInt8}, instance::PCInstance, Output: solution::Array{UInt8,n,n})

\item[-] \texttt{obj\_value.jl} : Compute the objective value given a solution, i.e. the minimal distance such that each point is in a distance smaller or equal to that from one of the center. (Input: solution::Array{UInt8}, instance::PCInstance, Output: solution::UInt16)

\item[-] \texttt{twoapprox\_heuristic.jl} : Function returning the 2-approximation heuristic of an instance (Input: instance::PCInstance, Output: solution::Array{UInt8,n})

\item[-] \texttt{random\_heuristic.jl} : Function returning a random solution of an instance (Input: instance::PCInstance, Output: solution::Array{UInt8,n})

\item[-] \texttt{bestHeuristicRandom.jl} : Returns the best solution out of 1000 random solutions (Input: instance::PCInstance, Output: solution::Array{UInt8,n})

\item[-] \texttt{bestHeuristicTwoApprox.jl} : Select the best solution out of 10 solutions found by the 2-approx heuristic (Input: instance::PCInstance, Output: solution::Array{UInt8,n})

\item[-] \texttt{bestHeuristicMix.jl} : Select the best solution out of 10 solutions found by the 2-approximation heuristic and 1000 random solutions. (Input: instance::PCInstance, Output: solution::Array{UInt8,n})

\item[-] \texttt{print\_solution.jl} : Print the position of the centers. (Input: instance::PCInstance, solution::Array{UInt8,n})

\item[-] \texttt{avg-divisor-search.py} :Python file to realize the grid search on the parameter ``divisor``

\end{itemize}

\section{Optimization}
\subsection{Hot starts}
Three hot starts have been tested: a random initialization, the 2-approximation algorithm and the default initialization implemented in the Gurobi Optimizer or Cbc, depending on which is used in the current execution.

\subsubsection{Random initialization}

The random initialization simply consists of choosing $p$ points to be the centers.

As this algorithm is not demanding in computational resources, this initialization is done $1000$ times and the best solution is kept as the actual initial solution for the program.

\subsubsection{2-approximation algorithm}

The 2-approximation algorithm, also called the farthest-first traversal, is an heuristic that guaranties the solution will not be further away than 2 times the optimal value of the objective function if the triangular inequalities is respected. 

The procedure is described in the pseudo-code here after. The first center is chosen randomly among the $n$ points. The minimal distance between each of the points left and the already chosen centers is computed: this gives us for each point the smallest distance it is located from any of the centers. This distance is then maximized in order to chose the next center. This procedure is repeated until the number of centers chosen is equal to $p$.

\begin{lstlisting}[mathescape=true]
(*\bfseries 2-approximation *) ($\pi$) :
    (*\bfseries input: *)  problem instance $\pi$
             the number of points $n$
             the number of centers $p$
    (*\bfseries output: *)  solution $sol$
    
    centers[p] = randomNumber(1,n)
    centersToFind = p - 1
    
    (*\bfseries while *) (centersToFind)
        outer_max = inf
        centerIdx = -1
        (*\bfseries for *) i = 1:n (*\bfseries and *) i  (*\bfseries not in *) centers :
            inner_min = findMinDistance(centers, instance)
            (*\bfseries if *) inner_min > outer_max
                outer_max = inner_min
                centerIdx = i
            (*\bfseries end *)
        (*\bfseries end *)
        centers[centersToFind--] = centerIdx
    (*\bfseries end *)
    sol = zeros(n)
    sol[centers] = 1
    (*\bfseries return *) $sol$
(*\bfseries end 2-approximation*)
\end{lstlisting}

In this case, it is not clear whether the distance metric considered has the triangular inequality property, this is why no assumptions on the optimally of this solution can be done. The name farthest-first traversal should thus be preferred. 

% https://pdfs.semanticscholar.org/f276/c00bac7594107c291947f560b7b48b1439d7.pdf
% https://cseweb.ucsd.edu/classes/sp11/cse202-a/lecture4-final.pdf
% http://algo2.iti.kit.edu/vanstee/courses/kcenter.pdf

\subsubsection{Default initialization}
This one was not implemented but was still part of the one explored as it is the default one when using an Optimizer and it yields good results, specially in the case of Gurobi.

\subsubsection{Comparison of the heuristics}

%%

\subsection{Number of variables}

\subsubsection{Dual problem}
The first idea to reduce the number of variables is to solve the dual problem. Indeed if a problem has $l$ variables and $m$ constraints, its dual problem has $m$ variables and $l$ constraints. If $l > m$, the dual problem does reduce the number of variables. Let's have a look at the formulations and their number of constraints and variables.

\begin{table}[h!]
\centering
\begin{tabular}{c|c|c}
    Formulation & \# variables & \# constraints \\
    \hline
    $P_1$ & $n^2 + n + 1$ & $n^2 + 2n + 2$ \\
    $P_3$ & $n + K$ & $nK+2$
\end{tabular}
\end{table}

Considering that $n$ and $K \in N$, we have that \#variables $<$ \#constraints. For $P_1$ this is obvious. For $P_2$, we have to solve the inequality $n + K > nK + 2$, which would indicate that taking the dual would reduce the number of variables. The solution of this inequality for n $>$ 1 is $K < \dfrac{n-2}{n-1}$ which is always smaller than 1. As K is an integer, this gives $K \leq 0$ which does not lead to any solution as $K > 0$ by definition. 

\subsection{Valid inequalities}

We used the Chv\'atal-Gomory procedure to add valid inequalities to the set of constraints.

To construct a valid inequality for the set $ X = \{x \in R^n_{+}: Ax \leq b \} \cap Z^n$, where $A$ is a $m \times n$ matrix with columns \{$a_j$, j = 1, \ldots, n\}, we write:
\begin{eqnarray*}
    \sum_{j=1}^n \left \lfloor{ua_j}\right \rfloor x_j & \leq & ub ~~ \text{where $u \in R^m_{+}$ and $x \geq 0$ is valid for P.}
\end{eqnarray*}

This inequation is used in constraint (\ref{eq:2}) of $P_1$ with a factor called $divisor$:
 
	\begin{eqnarray}
	 \sum_{j \in N} \left \lfloor{d_{ij}/divisor}\right \rfloor x_{ij} &\leq& z/divisor \label{eq:2_VI}
	\end{eqnarray}
	
We tried looking for a value of the divisor which would improve the performance as much as possible. It is clear that the strength of the inequality depends on how the floor operation \textit{cuts} the coefficients. So both the values in the distance matrix and the divisor have an important impact. Let us run our program using Cbc on the first instance to have some insight on how the divisor influences the execution time:

\input{optimizations-divisor-search}


We also tried using multiple divisors and add an ensemble of constraints. Again, this led to a great variety of results.

Let's however remark that using Gurobi, adding these valid inequalities does not have much effect: Gurobi itself already does implement operations (and actually, many more), so our valid inequality probably gets drowned into the pool of optimizations made by Gurobi. 

With Cbc, which is much more rudimentary, we could see astonishing improvements in the computational time (sometimes leading to more than 200\% speed-ups). Here is a plot of various divisor values used on the first three instances:

\input{opt-div-search-3}

There does not seem to be a global optimum for all instances, even though they have roughly the same distribution of distances. On instance 1, it seems to work great. On the two others, we have a curious behaviour: we have been asking ourselves why we could get times greater than before when adding a VI. We couldn't come up with a certain answer: perhaps our valid inequality is actually not valid, or it somehow makes Cbc shy away from the optimal solution, or again some other aspect we don't know of is involved. This is left as an open question.

To conclude on this attempt to optimize, we ran several other tests (see the performance comparison in conclusion) and observed that this VI implementation didn't actually help in most cases, considering all instances.

%\subsection{Cutting planes}


%%%
%%%
%%%
%%%
%%%
%%%

\section{Results}

First, let us show the optimal values we found for each instance:
\begin{itemize}
	\item \texttt{1.out}: 127
	\item \texttt{2.out}: 98
	\item \texttt{3.out}: 93
	\item \texttt{4.out}: 74
	\item \texttt{5.out}: 48
	\item \texttt{6.out}: 84
	\item \texttt{7.out}: 64
	\item \texttt{8.out}: 55
	\item \texttt{9.out}: 37
	\item \texttt{10.out}: 20
\end{itemize}

We had the same results with both formulations.

%Instance // Objective function // Example of solutions  // computational time (mean) //
\section{Conclusion}
	\subsection{Comparison of the two formulations}
	
	For P1, we have the following execution times, all computations leading to optimal solutions:
\input{time-all-instances-p1}

Note how we had to use a logarithmic scale for the vertical axis to keep the visualisation relevant. Indeed, Cbc could reach very long times compared to Gurobi.


We see that the instances requiring the longest execution time are \texttt{6.out} and \texttt{7.out}. Not only they have a large number of vertices ($N = $ 200), but also a very low $p$ value, respectively 5 and 10. Intuitively, this low $p$ combined with a high $N$ increase the difficulty of the search.


    Note how we had to use a logarithmic scale for the vertical axis to keep the visualisation relevant. Indeed, Cbc could reach very long times compared to Gurobi.

    We see that the instances requiring the longest execution time are \texttt{6.out} and \texttt{7.out}. Not only they have a large number of vertices ($N = $ 200), but also a very low $p$ value, respectively 5 and 10. Intuitively, this low $p$ combined with a high $N$ increase the difficulty of the search. as it means that more points have to be associated to less centers: the number of points per center increases and so does the optimal $\rho_K$.
	
	
\pagebreak

The same plot is shown for P3 in Figure 4.
\input{time-all-instances-p3}

Some values aren't present because they would simply require too much computational time. Any computation with Cbc on P3 without any optimization would take many hours to finish (6 hours for \texttt{1.out}), so they are not plotted here (they wouldn't compete with the others anyway). The same goes for Cbc on P3 with the additional VI as described before, and also for the farthest-first heuristic which also leads to hours of computation in this setup. Remark that being able to get some optimal solutions in a timely manner thanks to our implementations of heuristics is already a success.

To conclude on a high-level note, as long as we consider Gurobi, the change isn't so important: P3 is just a bit slower than P1. But for our implementations, the change is drastic: most of the time we aren't able to get optimal solutions in a decent computational time. This is likely to be thanks to the multiple optimizations implemented in Gurobi which make it more resistant to various formulations.



	\subsection{Wrap-up}
	This project gave us interesting insight on how to practically use solvers  and  in a way, how they worked internally. Comparing our own implementations with Gurobi was a challenging and exciting task as we could see our improvements make the execution times required to solve instances decrease and decrease over time.
	

\end{document}